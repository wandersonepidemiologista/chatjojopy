import streamlit as st
from langchain_community.llms import HuggingFaceHub
from langchain_huggingface import HuggingFaceEndpoint
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import os

# import random # Removido pois n√£o estava sendo utilizado

# --- Configura√ß√µes Iniciais ---

# ‚öñÔ∏è Carregar vari√°veis de ambiente (Requer arquivo .env com HUGGINGFACEHUB_API_TOKEN)
# Certifique-se de que o arquivo .env existe e cont√©m sua API key.
load_dotenv()

# üåê Configura√ß√£o da interface com tema personalizado
st.set_page_config(page_title="ChatJoJoPy", layout="wide")

# Estilo customizado com as cores fornecidas (CORRIGIDO)
st.markdown("""
    <style>
    body { background-color: #ffffff; color: #025e73; }
    .stApp { background-color: #ffffff; }
    .stTextInput > div > div > input { color: #025e73 !important; }
    /* CORRE√á√ÉO: Cor do texto ajustada para contraste com o fundo branco */
    .stChatMessage.user, .stChatMessage.assistant {
        background-color: #ffffff;
        border: 1px solid #e0e0e0; /* Adicionada borda sutil para melhor visualiza√ß√£o */
        border-radius: 10px;
        padding: 10px;
        margin: 5px 0;
        color: #025e73; /* Cor do texto corrigida */
    }
    .stButton button {
        background-color: #02735e !important;
        color: white !important;
    }
    .stButton button:hover {
        background-color: #015c4b !important; /* Escurecer um pouco no hover */
        color: white !important;
    }
    </style>
""", unsafe_allow_html=True)

# --- Sidebar ---
# Certifique-se que a pasta 'imagens' e o arquivo 'logo.png' existem
with st.sidebar:
    try:
        st.image("imagens/logo.png", width=200)
    except FileNotFoundError:
        st.error("Arquivo 'imagens/logo.png' n√£o encontrado.")
    st.title("**Chat JoJoPy**")
    st.subheader("Comunidade de m√£os dadas na resposta √†s emerg√™ncias em sa√∫de p√∫blica")
    st.markdown("""
    O termo **\"JOJOPY\"** tem origem no Gloss√°rio Ind√≠gena Guarani-Portugu√™s e significa **\"deram as m√£os\"**.
    No contexto guarani, essa express√£o vai muito al√©m de um gesto f√≠sico, simbolizando uni√£o e comunidade, representando a for√ßa dos la√ßos coletivos e a constru√ß√£o de uma sociedade solid√°ria.
    Essa simbologia traduz os objetivos esperados para os participantes do curso: **promover a coopera√ß√£o e o fortalecimento da coletividade**.
    """)

# --- Configura√ß√£o do Modelo e RAG ---

# üß† Inicializar modelo Hugging Face com LangChain
# Nota: 'google/flan-t5-small' √© um modelo text-to-text vers√°til e gratuito.
try:
    llm = HuggingFaceEndpoint(
        repo_id="facebook/bart-large-mnli",
        task="text2text-generation",
        huggingfacehub_api_token=os.environ.get("HUGGINGFACEHUB_API_TOKEN"),
        temperature=0.5,
        max_new_tokens=512
    )
except Exception as e:
    st.error(f"Erro ao inicializar o modelo LLM (HuggingFaceEndpoint) com BART: {e}")
    st.error("Verifique sua conex√£o e a chave HUGGINGFACEHUB_API_TOKEN no arquivo .env.")
    st.stop()

# üìÇ Carregar e indexar documentos
# Certifique-se que a pasta 'documentos' existe e cont√©m arquivos PDF.
@st.cache_resource # Usar cache para evitar recarregar/reindexar a cada intera√ß√£o
def carregar_e_indexar_base():
    """Carrega PDFs da pasta 'documentos', divide em chunks e cria um √≠ndice FAISS."""
    documentos = []
    pasta_documentos = "documentos"
    if not os.path.isdir(pasta_documentos):
        st.error(f"A pasta '{pasta_documentos}' n√£o foi encontrada.")
        return None # Retorna None se a pasta n√£o existe

    arquivos_pdf = [f for f in os.listdir(pasta_documentos) if f.lower().endswith(".pdf")]
    if not arquivos_pdf:
        st.warning(f"Nenhum arquivo PDF encontrado na pasta '{pasta_documentos}'. O chat funcionar√° sem base de conhecimento espec√≠fica.")
        return None # Retorna None se n√£o h√° PDFs

    for arquivo in arquivos_pdf:
        caminho = os.path.join(pasta_documentos, arquivo)
        try:
            loader = PyPDFLoader(caminho)
            documentos.extend(loader.load())
        except Exception as e:
            st.warning(f"Erro ao carregar o arquivo {arquivo}: {e}")
            continue # Pula para o pr√≥ximo arquivo em caso de erro

    if not documentos:
        st.error("Nenhum documento PDF p√¥de ser carregado com sucesso.")
        return None

    try:
        splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=150) # Ajuste overlap se necess√°rio
        chunks = splitter.split_documents(documentos)

        # Usar embeddings do HuggingFace (pode baixar modelo na primeira execu√ß√£o)
        embeddings = HuggingFaceEmbeddings()

        # Criar o √≠ndice FAISS
        db = FAISS.from_documents(chunks, embeddings)
        return db
    except Exception as e:
        st.error(f"Erro ao processar documentos e criar √≠ndice: {e}")
        return None

# Tentar carregar a base e configurar o RAG
db = carregar_e_indexar_base()
rag = None
if db:
    try:
        from langchain.prompts import PromptTemplate # Importe aqui dentro do bloco

        retriever = db.as_retriever(search_kwargs={'k': 3})
        # Criar um template de prompt customizado
        prompt_template = """Responda √† seguinte pergunta com base no contexto fornecido:
        {context}
        Pergunta: {question}
        Resposta:"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        # Configurar a chain RetrievalQA com o tipo 'stuff' e o prompt customizado
        rag = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    except Exception as e:
        st.error(f"Erro ao configurar a cadeia RAG: {e}")
else:
    st.warning("Base de documentos n√£o carregada. O chat pode n√£o ter informa√ß√µes espec√≠ficas dos arquivos.")

# --- Interface Principal com Abas ---
abas = st.tabs(["üß† Chat", "üìÑ Documentos", "‚öôÔ∏è Sobre"])

# Aba de Chat
with abas[0]:
    st.title("ü§ñ ChatJoJoPy ‚Äî Emerg√™ncias em Sa√∫de P√∫blica")
    st.markdown("Fa√ßa sua pergunta sobre os documentos carregados ou sobre emerg√™ncias em sa√∫de p√∫blica.")

    st.markdown("---")
    st.markdown("### üí° Exemplos de perguntas:")
    perguntas_exemplo = [
        "O que √© uma emerg√™ncia em sa√∫de p√∫blica?",
        "Quais s√£o os planos nacionais existentes?",
        "Qual a diferen√ßa entre prepara√ß√£o e resposta?",
        "O que √© um plano de conting√™ncia?",
        "Quais documentos orientam os munic√≠pios em desastres?",
        "O que s√£o as fases da resposta a emerg√™ncias?",
        "Como a vigil√¢ncia atua em situa√ß√µes de emerg√™ncia?",
        "Quais s√£o os indicadores de prontid√£o e resposta?"
    ]
    cols = st.columns(2)
    for i, pergunta in enumerate(perguntas_exemplo):
        cols[i % 2].markdown(f"- {pergunta}")
    st.markdown("---")


    # Inicializar hist√≥rico do chat na session state
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    # Mostrar hist√≥rico do chat
    for role, content in st.session_state.chat_history:
        with st.chat_message(role):
            st.write(content)

    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta aqui..."):
        # Adicionar mensagem do usu√°rio ao hist√≥rico e mostrar na tela
        st.session_state.chat_history.append(("user", prompt))
        with st.chat_message("user"):
            st.write(prompt)

        # Gerar resposta do assistente se o LLM foi inicializado
        if llm:
            try:
                with st.spinner("Pensando..."):
                    # Chamada direta ao LLM
                    resposta = llm.invoke(prompt)

                # Adicionar resposta do assistente ao hist√≥rico e mostrar na tela
                st.session_state.chat_history.append(("assistant", resposta))
                with st.chat_message("assistant"):
                    st.write(resposta)

            except Exception as e:
                resposta = f"Erro ao gerar resposta diretamente com o LLM: {e}. Verifique a configura√ß√£o e a API Key."
                print(f"Erro na execu√ß√£o do LLM direto: {e}")
        else:
            resposta = "O modelo de linguagem n√£o foi inicializado corretamente."
            st.session_state.chat_history.append(("assistant", resposta))
            with st.chat_message("assistant"):
                st.write(resposta)

# Aba de Documentos
with abas[1]:
    st.subheader("üìÑ Documentos Carregados")
    st.markdown("Os seguintes arquivos PDF foram encontrados na pasta `documentos` e podem ser baixados:")

    pasta_documentos = "documentos"
    try:
        if os.path.isdir(pasta_documentos):
            arquivos_pdf = [f for f in os.listdir(pasta_documentos) if f.lower().endswith(".pdf")]
            if arquivos_pdf:
                for arquivo in arquivos_pdf:
                    caminho = os.path.join(pasta_documentos, arquivo)
                    try:
                        with open(caminho, "rb") as f:
                            bytes_pdf = f.read()
                            st.download_button(
                                label=f"üîó Baixar: {arquivo}",
                                data=bytes_pdf,
                                file_name=arquivo,
                                mime="application/pdf"
                            )
                    except Exception as e:
                        st.error(f"N√£o foi poss√≠vel ler o arquivo {arquivo} para download: {e}")
            else:
                st.info("Nenhum arquivo PDF encontrado na pasta 'documentos'.")
        else:
             st.warning(f"A pasta '{pasta_documentos}' n√£o foi encontrada.")

    except Exception as e:
        st.error(f"Erro ao listar documentos: {e}")

# Aba Sobre
with abas[2]:
    st.subheader("‚öôÔ∏è Sobre o Projeto")
    st.markdown("""
    - **Nome:** Chat JoJoPy
    - **Objetivo:** Apoiar a consulta r√°pida a documentos t√©cnicos sobre emerg√™ncias em sa√∫de p√∫blica utilizando RAG (Retrieval-Augmented Generation).
    - **Tecnologias:** Streamlit, LangChain, Hugging Face Transformers.
    - **Desenvolvedor:** Wanderson Oliveira - Epidemiologista ([Sobre mim](https://www.wandersonepidemiologista.com/sobre))
    - **Saiba mais sobre o munic√≠pio fict√≠cio de Jojopy:** [E-Book Jojopy](https://epidemiologista.quarto.pub/jojopy/)
    """)
    st.markdown("---")
    st.markdown("*(Vers√£o do c√≥digo revisada e corrigida.)*")